Help on class Folder in module anomalib.data.datamodules.image.folder:

class Folder(anomalib.data.datamodules.base.image.AnomalibDataModule)
 |  Folder(name: str, normal_dir: str | pathlib.Path | collections.abc.Sequence[str | pathlib.Path], root: str | pathlib.Path | None = None, abnormal_dir: str | pathlib.Path | collections.abc.Sequence[str | pathlib.Path] | None = None, normal_test_dir: str | pathlib.Path | collections.abc.Sequence[str | pathlib.Path] | None = None, mask_dir: str | pathlib.Path | collections.abc.Sequence[str | pathlib.Path] | None = None, normal_split_ratio: float = 0.2, extensions: tuple[str] | None = None, train_batch_size: int = 32, eval_batch_size: int = 32, num_workers: int = 8, train_augmentations: torchvision.transforms.v2._transform.Transform | None = None, val_augmentations: torchvision.transforms.v2._transform.Transform | None = None, test_augmentations: torchvision.transforms.v2._transform.Transform | None = None, augmentations: torchvision.transforms.v2._transform.Transform | None = None, test_split_mode: anomalib.data.utils.split.TestSplitMode | str = <TestSplitMode.FROM_DIR: 'from_dir'>, test_split_ratio: float = 0.2, val_split_mode: anomalib.data.utils.split.ValSplitMode | str = <ValSplitMode.FROM_TEST: 'from_test'>, val_split_ratio: float = 0.5, seed: int | None = None) -> None
 |  
 |  Folder DataModule.
 |  
 |  Args:
 |      name (str): Name of the dataset. Used for logging/saving.
 |      normal_dir (str | Path | Sequence): Directory containing normal images.
 |      root (str | Path | None): Root folder containing normal and abnormal
 |          directories. Defaults to ``None``.
 |      abnormal_dir (str | Path | None | Sequence): Directory containing
 |          abnormal images. Defaults to ``None``.
 |      normal_test_dir (str | Path | Sequence | None): Directory containing
 |          normal test images. Defaults to ``None``.
 |      mask_dir (str | Path | Sequence | None): Directory containing mask
 |          annotations. Defaults to ``None``.
 |      normal_split_ratio (float): Ratio to split normal training images for
 |          test set when no normal test images exist.
 |          Defaults to ``0.2``.
 |      extensions (tuple[str, ...] | None): Image extensions to include.
 |          Defaults to ``None``.
 |      train_batch_size (int): Training batch size.
 |          Defaults to ``32``.
 |      eval_batch_size (int): Validation/test batch size.
 |          Defaults to ``32``.
 |      num_workers (int): Number of workers for data loading.
 |          Defaults to ``8``.
 |      train_augmentations (Transform | None): Augmentations to apply dto the training images
 |          Defaults to ``None``.
 |      val_augmentations (Transform | None): Augmentations to apply to the validation images.
 |          Defaults to ``None``.
 |      test_augmentations (Transform | None): Augmentations to apply to the test images.
 |          Defaults to ``None``.
 |      augmentations (Transform | None): General augmentations to apply if stage-specific
 |          augmentations are not provided.
 |      test_split_mode (TestSplitMode): Method to obtain test subset.
 |          Defaults to ``TestSplitMode.FROM_DIR``.
 |      test_split_ratio (float): Fraction of train images for testing.
 |          Defaults to ``0.2``.
 |      val_split_mode (ValSplitMode): Method to obtain validation subset.
 |          Defaults to ``ValSplitMode.FROM_TEST``.
 |      val_split_ratio (float): Fraction of images for validation.
 |          Defaults to ``0.5``.
 |      seed (int | None): Random seed for splitting.
 |          Defaults to ``None``.
 |  
 |  Example:
 |      Create and setup a folder datamodule::
 |  
 |          >>> from anomalib.data import Folder
 |          >>> datamodule = Folder(
 |          ...     name="custom",
 |          ...     root="./datasets/custom",
 |          ...     normal_dir="good",
 |          ...     abnormal_dir="defect",
 |          ...     mask_dir="mask"
 |          ... )
 |          >>> datamodule.setup()
 |  
 |      Get a batch from train dataloader::
 |  
 |          >>> batch = next(iter(datamodule.train_dataloader()))
 |          >>> batch.keys()
 |          dict_keys(['image', 'label', 'mask', 'image_path', 'mask_path'])
 |  
 |      Get a batch from test dataloader::
 |  
 |          >>> batch = next(iter(datamodule.test_dataloader()))
 |          >>> batch.keys()
 |          dict_keys(['image', 'label', 'mask', 'image_path', 'mask_path'])
 |  
 |  Method resolution order:
 |      Folder
 |      anomalib.data.datamodules.base.image.AnomalibDataModule
 |      lightning.pytorch.core.datamodule.LightningDataModule
 |      lightning.pytorch.core.hooks.DataHooks
 |      lightning.pytorch.core.mixins.hparams_mixin.HyperparametersMixin
 |      abc.ABC
 |      builtins.object
 |  
 |  Methods defined here:
 |  
 |  __init__(self, name: str, normal_dir: str | pathlib.Path | collections.abc.Sequence[str | pathlib.Path], root: str | pathlib.Path | None = None, abnormal_dir: str | pathlib.Path | collections.abc.Sequence[str | pathlib.Path] | None = None, normal_test_dir: str | pathlib.Path | collections.abc.Sequence[str | pathlib.Path] | None = None, mask_dir: str | pathlib.Path | collections.abc.Sequence[str | pathlib.Path] | None = None, normal_split_ratio: float = 0.2, extensions: tuple[str] | None = None, train_batch_size: int = 32, eval_batch_size: int = 32, num_workers: int = 8, train_augmentations: torchvision.transforms.v2._transform.Transform | None = None, val_augmentations: torchvision.transforms.v2._transform.Transform | None = None, test_augmentations: torchvision.transforms.v2._transform.Transform | None = None, augmentations: torchvision.transforms.v2._transform.Transform | None = None, test_split_mode: anomalib.data.utils.split.TestSplitMode | str = <TestSplitMode.FROM_DIR: 'from_dir'>, test_split_ratio: float = 0.2, val_split_mode: anomalib.data.utils.split.ValSplitMode | str = <ValSplitMode.FROM_TEST: 'from_test'>, val_split_ratio: float = 0.5, seed: int | None = None) -> None
 |      Attributes:
 |          prepare_data_per_node:
 |              If True, each LOCAL_RANK=0 will call prepare data.
 |              Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data.
 |          allow_zero_length_dataloader_with_multiple_devices:
 |              If True, dataloader with zero length within local rank is allowed.
 |              Default value is False.
 |  
 |  ----------------------------------------------------------------------
 |  Readonly properties defined here:
 |  
 |  name
 |      Get name of the datamodule.
 |      
 |      Returns:
 |          Name of the datamodule.
 |  
 |  ----------------------------------------------------------------------
 |  Data and other attributes defined here:
 |  
 |  __abstractmethods__ = frozenset()
 |  
 |  __annotations__ = {}
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from anomalib.data.datamodules.base.image.AnomalibDataModule:
 |  
 |  predict_dataloader(self) -> Any
 |      Get prediction dataloader.
 |      
 |      By default uses the test dataloader.
 |      
 |      Returns:
 |          DataLoader: Prediction dataloader
 |  
 |  setup(self, stage: str | None = None) -> None
 |      Set up train, validation and test data.
 |      
 |      This method handles the data splitting logic based on the configured
 |      modes.
 |      
 |      Args:
 |          stage (str | None): Current stage (fit/validate/test/predict).
 |              Defaults to ``None``.
 |  
 |  test_dataloader(self) -> Any
 |      Get test dataloader.
 |      
 |      Returns:
 |          DataLoader: Test dataloader
 |  
 |  train_dataloader(self) -> Any
 |      Get training dataloader.
 |      
 |      Returns:
 |          DataLoader: Training dataloader
 |  
 |  val_dataloader(self) -> Any
 |      Get validation dataloader.
 |      
 |      Returns:
 |          DataLoader: Validation dataloader
 |  
 |  ----------------------------------------------------------------------
 |  Class methods inherited from anomalib.data.datamodules.base.image.AnomalibDataModule:
 |  
 |  from_config(config_path: str | pathlib.Path, **kwargs) -> 'AnomalibDataModule'
 |      Create datamodule instance from config file.
 |      
 |      Args:
 |          config_path (str | Path): Path to config file
 |          **kwargs: Additional args to override config
 |      
 |      Returns:
 |          AnomalibDataModule: Instantiated datamodule
 |      
 |      Raises:
 |          FileNotFoundError: If config file not found
 |          ValueError: If instantiated object is not AnomalibDataModule
 |      
 |      Example:
 |          Load from config file::
 |      
 |              >>> config_path = "examples/configs/data/mvtec.yaml"
 |              >>> datamodule = AnomalibDataModule.from_config(config_path)
 |      
 |          Override config values::
 |      
 |              >>> datamodule = AnomalibDataModule.from_config(
 |              ...     config_path,
 |              ...     data_train_batch_size=8
 |              ... )
 |  
 |  ----------------------------------------------------------------------
 |  Readonly properties inherited from anomalib.data.datamodules.base.image.AnomalibDataModule:
 |  
 |  task
 |      Get the task type.
 |      
 |      Returns:
 |          TaskType: Type of anomaly task (classification/segmentation)
 |      
 |      Raises:
 |          AttributeError: If no datasets have been set up yet
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from anomalib.data.datamodules.base.image.AnomalibDataModule:
 |  
 |  category
 |      Get dataset category name.
 |      
 |      Returns:
 |          str: Name of the current category
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from lightning.pytorch.core.datamodule.LightningDataModule:
 |  
 |  __str__(self) -> str
 |      Return a string representation of the datasets that are set up.
 |      
 |      Returns:
 |          A string representation of the datasets that are setup.
 |  
 |  load_state_dict(self, state_dict: dict[str, typing.Any]) -> None
 |      Called when loading a checkpoint, implement to reload datamodule state given datamodule state_dict.
 |      
 |      Args:
 |          state_dict: the datamodule state returned by ``state_dict``.
 |  
 |  on_exception(self, exception: BaseException) -> None
 |      Called when the trainer execution is interrupted by an exception.
 |  
 |  state_dict(self) -> dict[str, typing.Any]
 |      Called when saving a checkpoint, implement to generate and save datamodule state.
 |      
 |      Returns:
 |          A dictionary containing datamodule state.
 |  
 |  ----------------------------------------------------------------------
 |  Class methods inherited from lightning.pytorch.core.datamodule.LightningDataModule:
 |  
 |  from_datasets(train_dataset: Union[torch.utils.data.dataset.Dataset, collections.abc.Iterable[torch.utils.data.dataset.Dataset], NoneType] = None, val_dataset: Union[torch.utils.data.dataset.Dataset, collections.abc.Iterable[torch.utils.data.dataset.Dataset], NoneType] = None, test_dataset: Union[torch.utils.data.dataset.Dataset, collections.abc.Iterable[torch.utils.data.dataset.Dataset], NoneType] = None, predict_dataset: Union[torch.utils.data.dataset.Dataset, collections.abc.Iterable[torch.utils.data.dataset.Dataset], NoneType] = None, batch_size: int = 1, num_workers: int = 0, **datamodule_kwargs: Any) -> 'LightningDataModule'
 |      Create an instance from torch.utils.data.Dataset.
 |      
 |      Args:
 |          train_dataset: Optional dataset or iterable of datasets to be used for train_dataloader()
 |          val_dataset: Optional dataset or iterable of datasets to be used for val_dataloader()
 |          test_dataset: Optional dataset or iterable of datasets to be used for test_dataloader()
 |          predict_dataset: Optional dataset or iterable of datasets to be used for predict_dataloader()
 |          batch_size: Batch size to use for each dataloader. Default is 1. This parameter gets forwarded to the
 |              ``__init__`` if the datamodule has such a name defined in its signature.
 |          num_workers: Number of subprocesses to use for data loading. 0 means that the
 |              data will be loaded in the main process. Number of CPUs available. This parameter gets forwarded to the
 |              ``__init__`` if the datamodule has such a name defined in its signature.
 |          **datamodule_kwargs: Additional parameters that get passed down to the datamodule's ``__init__``.
 |  
 |  load_from_checkpoint(cls, checkpoint_path: Union[str, pathlib.Path, IO], map_location: Union[torch.device, str, int, Callable[[torch.storage.UntypedStorage, str], Optional[torch.storage.UntypedStorage]], dict[Union[torch.device, str, int], Union[torch.device, str, int]], NoneType] = None, hparams_file: Union[str, pathlib.Path, NoneType] = None, weights_only: Optional[bool] = None, **kwargs: Any) -> Self
 |      Primary way of loading a datamodule from a checkpoint. When Lightning saves a checkpoint it stores the
 |      arguments passed to ``__init__``  in the checkpoint under ``"datamodule_hyper_parameters"``.
 |      
 |      Any arguments specified through \*\*kwargs will override args stored in ``"datamodule_hyper_parameters"``.
 |      
 |      Args:
 |          checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object
 |          map_location:
 |              If your checkpoint saved a GPU model and you now load on CPUs
 |              or a different number of GPUs, use this to map to the new setup.
 |              The behaviour is the same as in :func:`torch.load`.
 |          hparams_file: Optional path to a ``.yaml`` or ``.csv`` file with hierarchical structure
 |              as in this example::
 |      
 |                  dataloader:
 |                      batch_size: 32
 |      
 |              You most likely won't need this since Lightning will always save the hyperparameters
 |              to the checkpoint.
 |              However, if your checkpoint weights don't have the hyperparameters saved,
 |              use this method to pass in a ``.yaml`` file with the hparams you'd like to use.
 |              These will be converted into a :class:`~dict` and passed into your
 |              :class:`LightningDataModule` for use.
 |      
 |              If your datamodule's ``hparams`` argument is :class:`~argparse.Namespace`
 |              and ``.yaml`` file has hierarchical structure, you need to refactor your datamodule to treat
 |              ``hparams`` as :class:`~dict`.
 |          weights_only: If ``True``, restricts loading to ``state_dicts`` of plain ``torch.Tensor`` and other
 |              primitive types. If loading a checkpoint from a trusted source that contains an ``nn.Module``, use
 |              ``weights_only=False``. If loading checkpoint from an untrusted source, we recommend using
 |              ``weights_only=True``. For more information, please refer to the
 |              `PyTorch Developer Notes on Serialization Semantics <https://docs.pytorch.org/docs/main/notes/serialization.html#id3>`_.
 |          \**kwargs: Any extra keyword args needed to init the datamodule. Can also be used to override saved
 |              hyperparameter values.
 |      
 |      Return:
 |          :class:`LightningDataModule` instance with loaded weights and hyperparameters (if available).
 |      
 |      Note:
 |          ``load_from_checkpoint`` is a **class** method. You must use your :class:`LightningDataModule`
 |          **class** to call it instead of the :class:`LightningDataModule` instance, or a
 |          ``TypeError`` will be raised.
 |      
 |      Example::
 |      
 |          # load weights without mapping ...
 |          datamodule = MyLightningDataModule.load_from_checkpoint('path/to/checkpoint.ckpt')
 |      
 |          # or load weights and hyperparameters from separate files.
 |          datamodule = MyLightningDataModule.load_from_checkpoint(
 |              'path/to/checkpoint.ckpt',
 |              hparams_file='/path/to/hparams_file.yaml'
 |          )
 |      
 |          # override some of the params with new values
 |          datamodule = MyLightningDataModule.load_from_checkpoint(
 |              PATH,
 |              batch_size=32,
 |              num_workers=10,
 |          )
 |  
 |  ----------------------------------------------------------------------
 |  Data and other attributes inherited from lightning.pytorch.core.datamodule.LightningDataModule:
 |  
 |  CHECKPOINT_HYPER_PARAMS_KEY = 'datamodule_hyper_parameters'
 |  
 |  CHECKPOINT_HYPER_PARAMS_NAME = 'datamodule_hparams_name'
 |  
 |  CHECKPOINT_HYPER_PARAMS_TYPE = 'datamodule_hparams_type'
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from lightning.pytorch.core.hooks.DataHooks:
 |  
 |  on_after_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any
 |      Override to alter or apply batch augmentations to your batch after it is transferred to the device.
 |      
 |      Note:
 |          To check the current state of execution of this hook you can use
 |          ``self.trainer.training/testing/validating/predicting`` so that you can
 |          add different logic as per your requirement.
 |      
 |      Args:
 |          batch: A batch of data that needs to be altered or augmented.
 |          dataloader_idx: The index of the dataloader to which the batch belongs.
 |      
 |      Returns:
 |          A batch of data
 |      
 |      Example::
 |      
 |          def on_after_batch_transfer(self, batch, dataloader_idx):
 |              batch['x'] = gpu_transforms(batch['x'])
 |              return batch
 |      
 |      See Also:
 |          - :meth:`on_before_batch_transfer`
 |          - :meth:`transfer_batch_to_device`
 |  
 |  on_before_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any
 |      Override to alter or apply batch augmentations to your batch before it is transferred to the device.
 |      
 |      Note:
 |          To check the current state of execution of this hook you can use
 |          ``self.trainer.training/testing/validating/predicting`` so that you can
 |          add different logic as per your requirement.
 |      
 |      Args:
 |          batch: A batch of data that needs to be altered or augmented.
 |          dataloader_idx: The index of the dataloader to which the batch belongs.
 |      
 |      Returns:
 |          A batch of data
 |      
 |      Example::
 |      
 |          def on_before_batch_transfer(self, batch, dataloader_idx):
 |              batch['x'] = transforms(batch['x'])
 |              return batch
 |      
 |      See Also:
 |          - :meth:`on_after_batch_transfer`
 |          - :meth:`transfer_batch_to_device`
 |  
 |  prepare_data(self) -> None
 |      Use this to download and prepare data. Downloading and saving data with multiple processes (distributed
 |      settings) will result in corrupted data. Lightning ensures this method is called only within a single process,
 |      so you can safely add your downloading logic within.
 |      
 |      .. warning:: DO NOT set state to the model (use ``setup`` instead)
 |          since this is NOT called on every device
 |      
 |      Example::
 |      
 |          def prepare_data(self):
 |              # good
 |              download_data()
 |              tokenize()
 |              etc()
 |      
 |              # bad
 |              self.split = data_split
 |              self.some_state = some_other_state()
 |      
 |      In a distributed environment, ``prepare_data`` can be called in two ways
 |      (using :ref:`prepare_data_per_node<common/lightning_module:prepare_data_per_node>`)
 |      
 |      1. Once per node. This is the default and is only called on LOCAL_RANK=0.
 |      2. Once in total. Only called on GLOBAL_RANK=0.
 |      
 |      Example::
 |      
 |          # DEFAULT
 |          # called once per node on LOCAL_RANK=0 of that node
 |          class LitDataModule(LightningDataModule):
 |              def __init__(self):
 |                  super().__init__()
 |                  self.prepare_data_per_node = True
 |      
 |      
 |          # call on GLOBAL_RANK=0 (great for shared file systems)
 |          class LitDataModule(LightningDataModule):
 |              def __init__(self):
 |                  super().__init__()
 |                  self.prepare_data_per_node = False
 |      
 |      This is called before requesting the dataloaders:
 |      
 |      .. code-block:: python
 |      
 |          model.prepare_data()
 |          initialize_distributed()
 |          model.setup(stage)
 |          model.train_dataloader()
 |          model.val_dataloader()
 |          model.test_dataloader()
 |          model.predict_dataloader()
 |  
 |  teardown(self, stage: str) -> None
 |      Called at the end of fit (train + validate), validate, test, or predict.
 |      
 |      Args:
 |          stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'``
 |  
 |  transfer_batch_to_device(self, batch: Any, device: torch.device, dataloader_idx: int) -> Any
 |      Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data
 |      structure.
 |      
 |      The data types listed below (and any arbitrary nesting of them) are supported out of the box:
 |      
 |      - :class:`torch.Tensor` or anything that implements `.to(...)`
 |      - :class:`list`
 |      - :class:`dict`
 |      - :class:`tuple`
 |      
 |      For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...).
 |      
 |      Note:
 |          This hook should only transfer the data and not modify it, nor should it move the data to
 |          any other device than the one passed in as argument (unless you know what you are doing).
 |          To check the current state of execution of this hook you can use
 |          ``self.trainer.training/testing/validating/predicting`` so that you can
 |          add different logic as per your requirement.
 |      
 |      Args:
 |          batch: A batch of data that needs to be transferred to a new device.
 |          device: The target device as defined in PyTorch.
 |          dataloader_idx: The index of the dataloader to which the batch belongs.
 |      
 |      Returns:
 |          A reference to the data on the new device.
 |      
 |      Example::
 |      
 |          def transfer_batch_to_device(self, batch, device, dataloader_idx):
 |              if isinstance(batch, CustomBatch):
 |                  # move all tensors in your custom data structure to the device
 |                  batch.samples = batch.samples.to(device)
 |                  batch.targets = batch.targets.to(device)
 |              elif dataloader_idx == 0:
 |                  # skip device transfer for the first dataloader or anything you wish
 |                  pass
 |              else:
 |                  batch = super().transfer_batch_to_device(batch, device, dataloader_idx)
 |              return batch
 |      
 |      See Also:
 |          - :meth:`move_data_to_device`
 |          - :meth:`apply_to_collection`
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from lightning.pytorch.core.hooks.DataHooks:
 |  
 |  __dict__
 |      dictionary for instance variables
 |  
 |  __weakref__
 |      list of weak references to the object
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from lightning.pytorch.core.mixins.hparams_mixin.HyperparametersMixin:
 |  
 |  save_hyperparameters(self, *args: Any, ignore: Union[collections.abc.Sequence[str], str, NoneType] = None, frame: Optional[frame] = None, logger: bool = True) -> None
 |      Save arguments to ``hparams`` attribute.
 |      
 |      Args:
 |          args: single object of `dict`, `NameSpace` or `OmegaConf`
 |              or string names or arguments from class ``__init__``
 |          ignore: an argument name or a list of argument names from
 |              class ``__init__`` to be ignored
 |          frame: a frame object. Default is None
 |          logger: Whether to send the hyperparameters to the logger. Default: True
 |      
 |      Example::
 |          >>> from lightning.pytorch.core.mixins import HyperparametersMixin
 |          >>> class ManuallyArgsModel(HyperparametersMixin):
 |          ...     def __init__(self, arg1, arg2, arg3):
 |          ...         super().__init__()
 |          ...         # manually assign arguments
 |          ...         self.save_hyperparameters('arg1', 'arg3')
 |          ...     def forward(self, *args, **kwargs):
 |          ...         ...
 |          >>> model = ManuallyArgsModel(1, 'abc', 3.14)
 |          >>> model.hparams
 |          "arg1": 1
 |          "arg3": 3.14
 |      
 |          >>> from lightning.pytorch.core.mixins import HyperparametersMixin
 |          >>> class AutomaticArgsModel(HyperparametersMixin):
 |          ...     def __init__(self, arg1, arg2, arg3):
 |          ...         super().__init__()
 |          ...         # equivalent automatic
 |          ...         self.save_hyperparameters()
 |          ...     def forward(self, *args, **kwargs):
 |          ...         ...
 |          >>> model = AutomaticArgsModel(1, 'abc', 3.14)
 |          >>> model.hparams
 |          "arg1": 1
 |          "arg2": abc
 |          "arg3": 3.14
 |      
 |          >>> from lightning.pytorch.core.mixins import HyperparametersMixin
 |          >>> class SingleArgModel(HyperparametersMixin):
 |          ...     def __init__(self, params):
 |          ...         super().__init__()
 |          ...         # manually assign single argument
 |          ...         self.save_hyperparameters(params)
 |          ...     def forward(self, *args, **kwargs):
 |          ...         ...
 |          >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14))
 |          >>> model.hparams
 |          "p1": 1
 |          "p2": abc
 |          "p3": 3.14
 |      
 |          >>> from lightning.pytorch.core.mixins import HyperparametersMixin
 |          >>> class ManuallyArgsModel(HyperparametersMixin):
 |          ...     def __init__(self, arg1, arg2, arg3):
 |          ...         super().__init__()
 |          ...         # pass argument(s) to ignore as a string or in a list
 |          ...         self.save_hyperparameters(ignore='arg2')
 |          ...     def forward(self, *args, **kwargs):
 |          ...         ...
 |          >>> model = ManuallyArgsModel(1, 'abc', 3.14)
 |          >>> model.hparams
 |          "arg1": 1
 |          "arg3": 3.14
 |  
 |  ----------------------------------------------------------------------
 |  Readonly properties inherited from lightning.pytorch.core.mixins.hparams_mixin.HyperparametersMixin:
 |  
 |  hparams
 |      The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For
 |      the frozen set of initial hyperparameters, use :attr:`hparams_initial`.
 |      
 |      Returns:
 |          Mutable hyperparameters dictionary
 |  
 |  hparams_initial
 |      The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only.
 |      Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`.
 |      
 |      Returns:
 |          AttributeDict: immutable initial hyperparameters
 |  
 |  ----------------------------------------------------------------------
 |  Data and other attributes inherited from lightning.pytorch.core.mixins.hparams_mixin.HyperparametersMixin:
 |  
 |  __jit_unused_properties__ = ['hparams', 'hparams_initial']

